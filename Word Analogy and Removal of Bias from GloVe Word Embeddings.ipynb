{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "### v1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cWEywjgpJWhM",
    "outputId": "6b0b856a-d05d-4a6d-ba94-01d247254f20"
   },
   "outputs": [],
   "source": [
    "# Importing the necessary packages\n",
    "\n",
    "import numpy as np\n",
    "from w2v_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xCsAgH3dJWhO"
   },
   "outputs": [],
   "source": [
    "# We will use 50-dimensional GloVe vectors to represent words\n",
    "\n",
    "words, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')\n",
    "\n",
    "# words - A set of words in the vocabulary\n",
    "# word_to_vec_map - A dictionary mapping words to their GloVe vector representation\n",
    "\n",
    "# print(type(words))\n",
    "# print(type(word_to_vec_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion of Word Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mNjuTQ5JJWhP"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similarity between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity \n",
    "    \"\"\"\n",
    "    \n",
    "    # Special case - We consider the case u = [0, 0], v=[0, 0]\n",
    "    if np.all(u == v):\n",
    "        return 1\n",
    "    \n",
    "    # Compute the dot product between u and v (≈1 line)\n",
    "    dot = np.dot(u,v)\n",
    "    # Compute the L2 norm of u (≈1 line)\n",
    "    norm_u = np.sqrt(np.sum(np.dot(u,u)))\n",
    "    \n",
    "    # Compute the L2 norm of v (≈1 line)\n",
    "    norm_v = np.sqrt(np.sum(np.dot(v,v)))\n",
    "    \n",
    "    # Avoid division by 0\n",
    "    # If the difference between norm_u * norm_v and 0 is less than the absolute tolerance = 1e-32, then return 0\n",
    "    # If the above case, then either one of them is a zero vector and the cosine_similarity between a zero vector and the\n",
    "    # other given vector = 0. A zero vector is orthogonal to any other vector.\n",
    "    if np.isclose(norm_u * norm_v, 0, atol=1e-32):\n",
    "        return 0\n",
    "    \n",
    "    # Now we compute the cosine similarity\n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kGBV3yoQJWhS"
   },
   "outputs": [],
   "source": [
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
    "    \n",
    "    Arguments:\n",
    "    word_a -- a word, string\n",
    "    word_b -- a word, string\n",
    "    word_c -- a word, string\n",
    "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    # First we convert words to lowercase\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    # Get the word embeddings e_a, e_b and e_c\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "    \n",
    "    words = word_to_vec_map.keys()\n",
    "    # We initialize max_consine_similarity to be a large negative number\n",
    "    max_cosine_sim = -100 \n",
    "    # We initialize the best word with None\n",
    "    best_word = None    \n",
    "    \n",
    "    # We loop over the whole word vector set\n",
    "    for w in words:   \n",
    "        # To avoid best_word being one the input words, we skip the input word_c\n",
    "        # We skip word_c from query\n",
    "        if w == word_c:\n",
    "            continue\n",
    "        \n",
    "        # Computing cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  \n",
    "        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word \n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian :: spain -> spanish\n",
      "india -> delhi :: japan -> tokyo\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: large -> smaller\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad, word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMRD25MuJWhW"
   },
   "source": [
    "### Debiasing Word Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_qpU-C3KJWhW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07656667  0.34967667 -0.40057667 -0.03130333  0.0088      0.72586333\n",
      "  0.10256     0.14906333  0.4780662  -0.22850987  0.05957667 -0.68663\n",
      "  0.62210033  0.10395     0.17747667  0.09556867 -0.49258333 -0.17066233\n",
      "  0.46930033  0.02196333  0.28145667  0.50513333  0.17144733  0.40154767\n",
      "  0.24039333  0.1646     -0.17984667  0.24042667  0.05689333 -0.31423\n",
      " -0.10933333  0.26355967  0.06100667 -0.01156405 -0.12236333 -0.188245\n",
      " -0.13215057 -0.068186    0.05624667 -0.29555567 -0.09669533 -0.29559667\n",
      "  0.62465867 -0.40130167  0.03330667 -0.24831667  0.26381667 -0.28738333\n",
      "  0.03020433  0.054106  ]\n"
     ]
    }
   ],
   "source": [
    "# Calculating the 'Bias' direction\n",
    "\n",
    "e1 = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "e2 = word_to_vec_map['mother'] - word_to_vec_map['father']\n",
    "e3 = word_to_vec_map['girl'] - word_to_vec_map['boy']\n",
    "g = (e1 + e2 + e3)/3\n",
    "print(g)\n",
    "\n",
    "# 'g' is an approximate representation of 'gender'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TgqV6pDxJWhX",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of names and their similarities with constructed vector:\n",
      "john -0.30873091089769905\n",
      "marie 0.34257515107827113\n",
      "sophie 0.4116200252265308\n",
      "ronaldo -0.29083978511732383\n",
      "priya 0.1964679344860046\n",
      "rahul -0.194921476386334\n",
      "danielle 0.2923957653171286\n",
      "reza -0.1679382162425299\n",
      "katy 0.3113243060566435\n",
      "yasmin 0.19658379893678699\n"
     ]
    }
   ],
   "source": [
    "print ('List of names and their similarities with constructed vector:')\n",
    "\n",
    "# girls and boys name\n",
    "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
    "\n",
    "for w in name_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above case, we observe that female first names tend to have a positive cosine similarity with our constructed vector, \n",
    "# while male first names tend to have a negative cosine similarity and this seems acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying with other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wgadfCaGJWhY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other words and their similarities:\n",
      "lipstick 0.4136681512625245\n",
      "guns -0.08755154639507806\n",
      "science -0.058374259643848236\n",
      "arts 0.01176046812578374\n",
      "literature 0.023169467893751714\n",
      "warrior -0.16564638100307946\n",
      "doctor 0.07721412726656676\n",
      "tree 0.035380421070982306\n",
      "receptionist 0.30167259871100655\n",
      "technology -0.1619210846255818\n",
      "fashion 0.1416547219136271\n",
      "teacher 0.10545901736578718\n",
      "engineer -0.22639944157426764\n",
      "pilot -0.03699357317847414\n",
      "computer -0.1682103192173514\n",
      "singer 0.20093000793226248\n"
     ]
    }
   ],
   "source": [
    "print('Other words and their similarities:')\n",
    "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
    "for w in word_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first make the embeddings of words to have L2 norm = 1\n",
    "\n",
    "word_to_vec_map_unit_vectors = {}\n",
    "for word in word_to_vec_map.keys():\n",
    "    embedding = word_to_vec_map[word]\n",
    "    word_to_vec_map_unit_vectors[word] = embedding/np.linalg.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01506562  0.05674597 -0.06807928  0.00160754 -0.014503    0.12191253\n",
      "  0.02812728  0.02154482  0.08825992 -0.04282351  0.00805539 -0.12824188\n",
      "  0.11472482  0.0173499   0.02193565  0.02041312 -0.08110868 -0.0375994\n",
      "  0.0869276  -0.00104698  0.05802643  0.07449733  0.03111171  0.06634303\n",
      "  0.03654503  0.05869284 -0.0254643   0.04347939  0.00634609 -0.05206027\n",
      " -0.04707763  0.05191345  0.01418016 -0.00448297 -0.02789399 -0.03850989\n",
      " -0.0247095  -0.00942581  0.00506779 -0.04515941 -0.01451673 -0.0586928\n",
      "  0.11671609 -0.07113848 -0.00264706 -0.03825909  0.05018548 -0.0357483\n",
      "  0.00161449  0.01005749]\n"
     ]
    }
   ],
   "source": [
    "e1 = word_to_vec_map_unit_vectors['woman'] - word_to_vec_map_unit_vectors['man']\n",
    "e2 = word_to_vec_map_unit_vectors['mother'] - word_to_vec_map_unit_vectors['father']\n",
    "e3 = word_to_vec_map_unit_vectors['girl'] - word_to_vec_map_unit_vectors['boy']\n",
    "g_unit = (e1 + e2 + e3)/3\n",
    "print(g_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "79Pk0QDhJWhZ"
   },
   "outputs": [],
   "source": [
    "def neutralize(word, g, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Removes the bias of \"word\" by projecting it on the space orthogonal to the bias axis. \n",
    "    This function ensures that gender neutral words are zero in the gender subspace.\n",
    "    \n",
    "    Arguments:\n",
    "        word -- string indicating the word to debias\n",
    "        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)\n",
    "        word_to_vec_map -- dictionary mapping words to their corresponding vectors.\n",
    "    \n",
    "    Returns:\n",
    "        e_debiased -- neutralized word vector representation of the input \"word\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # We select the word vector representation of \"word\" using the word_to_vec_map\n",
    "    e = word_to_vec_map[word]\n",
    "    \n",
    "    # We compute e_biascomponent \n",
    "    e_biascomponent = np.linalg.norm(e) * cosine_similarity(e,g) * (g/np.linalg.norm(g))\n",
    " \n",
    "    # We neutralize e by subtracting e_biascomponent from it \n",
    "    # e_debiased should be equal to its orthogonal projection.\n",
    "    e_debiased = e - e_biascomponent\n",
    "    \n",
    "    return e_debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6PgkxwxXJWhZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between receptionist and g, before neutralizing:  0.30167259871100655\n",
      "cosine similarity between receptionist and g_unit, after neutralizing:  -7.560738707307337e-17\n"
     ]
    }
   ],
   "source": [
    "word = \"receptionist\"\n",
    "print(\"cosine similarity between \" + word + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[word], g))\n",
    "\n",
    "e_debiased = neutralize(word, g_unit, word_to_vec_map_unit_vectors)\n",
    "print(\"cosine similarity between \" + word + \" and g_unit, after neutralizing: \", cosine_similarity(e_debiased, g_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above case, we observe that the second result is close to 0. (i.e e_debiased and g_unit are almost orthogonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aBhxJtGIJWha"
   },
   "outputs": [],
   "source": [
    "def equalize(pair, bias_axis, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Debias gender specific words by following the equalize method described in the figure above.\n",
    "    \n",
    "    Arguments:\n",
    "    pair -- pair of strings of gender specific words to debias, e.g. (\"actress\", \"actor\") \n",
    "    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender\n",
    "    word_to_vec_map -- dictionary mapping words to their corresponding vectors\n",
    "    \n",
    "    Returns\n",
    "    e_1 -- word vector corresponding to the first word\n",
    "    e_2 -- word vector corresponding to the second word\n",
    "    \"\"\"\n",
    "    \n",
    "    # Selecting word vector representation of \"word\" using word_to_vec_map\n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n",
    "    \n",
    "    # Computing the mean of e_w1 and e_w2\n",
    "    mu = (e_w1 + e_w2)/2\n",
    "\n",
    "    # Computing the projections of mu over the bias axis and the orthogonal axis\n",
    "    mu_B = (np.dot(mu, bias_axis)/np.linalg.norm(bias_axis)) * (bias_axis) / np.linalg.norm(bias_axis)\n",
    "    mu_orth = mu - mu_B\n",
    "\n",
    "    # Compute e_w1B and e_w2B (i.e The components of e_w1 and e_w2 along the bias_axis)\n",
    "    e_w1B = (np.dot(e_w1, bias_axis)/np.linalg.norm(bias_axis)) * (bias_axis) / np.linalg.norm(bias_axis)\n",
    "    e_w2B = (np.dot(e_w2, bias_axis)/np.linalg.norm(bias_axis)) * (bias_axis) / np.linalg.norm(bias_axis)\n",
    "        \n",
    "    # Recentering the 'e_w1B' and 'e_w2B' such that 'corrected_e_w1B' and 'corrected_e_w2B' are equidistant from the \n",
    "    # bias axis and hence equidistant from the 'neutralized' words.\n",
    "    corrected_e_w1B = np.sqrt(1-(np.linalg.norm(mu_orth)**2)) * (e_w1B - mu_B)/np.linalg.norm(e_w1B - mu_B)\n",
    "    corrected_e_w2B = np.sqrt(1-(np.linalg.norm(mu_orth)**2)) * (e_w2B - mu_B)/np.linalg.norm(e_w2B - mu_B)\n",
    "\n",
    "    # Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)\n",
    "    e1 = corrected_e_w1B + mu_orth\n",
    "    e2 = corrected_e_w2B + mu_orth\n",
    "                                                                \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return e1, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note\n",
    "\n",
    "# After 'Neutralization', words like 'babysitter', 'nurse', 'doctor', 'homemaker' would lie on\n",
    "# the 49-dimensional axis which is orthogonal to the bias axis.\n",
    "\n",
    "# The 'corrected_e_w1B' and 'corrected_e_w2B' help in recentering the 'e_w1B' and 'e_w2B' such that 'corrected_e_w1B' and \n",
    "# 'corrected_e_w2B' are equidistant from the bias axis and hence equidistant from the 'neutralized' words.\n",
    "\n",
    "# The resultant 'e1' and 'e2' are such that they have undergone equalization (i.e now they are equidistant from the bias axis) \n",
    "# and their meanings have not changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "P405J5ZSJWhb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities before equalizing:\n",
      "cosine_similarity(word_to_vec_map[\"man\"], gender) =  -0.024358754123475775\n",
      "cosine_similarity(word_to_vec_map[\"woman\"], gender) =  0.3979047171251496\n",
      "\n",
      "cosine similarities after equalizing:\n",
      "cosine_similarity(e1, gender) =  -0.24211016258888443\n",
      "cosine_similarity(e2, gender) =  0.24211016258888443\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"], g))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"], g))\n",
    "print()\n",
    "e1, e2 = equalize((\"man\", \"woman\"), g_unit, word_to_vec_map_unit_vectors)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, g_unit))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, g_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the magnitude of angles between the bias axis and the embedding vectors 'e1' and 'e2' are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY2ZhorNJWhd"
   },
   "source": [
    "###  References\n",
    "\n",
    "- The debiasing algorithm is from Bolukbasi et al., 2016, [Man is to Computer Programmer as Woman is to\n",
    "Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n",
    "- The GloVe word embeddings were due to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "DLSC5W2-A1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
